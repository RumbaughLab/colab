{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RumbaughLab_psyTrack.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "bUcvebn-BE5h",
        "MVTpgUuvCsg7"
      ],
      "authorship_tag": "ABX9TyMu6ETu3arOtfi7/jVZ+yY2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RumbaughLab/colab/blob/main/RumbaughLab_psyTrack.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# _Implementation of psyTrack for Rumbaugh Lab_\n",
        "## Figure Generator\n",
        "\n",
        "relevant to:\n",
        "1. Cris Creson\n",
        "2. Sheldon Michaelson\n",
        "3. Randy Golvin\n",
        "4. Tom Vaissiere\n",
        "\n",
        "\n",
        "_(v1.0, last updated January, 26, 2022)_\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "3cl2R3bwztnV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook will attempt at the implementation of psyTrack on WDIL data and especially the WDIL dataset WDIL0007 which are located in a shared folder.\n",
        "Although this has been edited in colab it is not meant to be use in colab but rather to be run locally as a jupyter notebook as the data can be accessed through Scripps Network without long upload time.\n",
        "\n",
        "Important note: modification of the inuputs should enable analysis of other WDIL dataset\n",
        "\n",
        "Several additional layres can be incorporated like:\n",
        "\n",
        "\n",
        "1.   Stimulus intensities\n",
        "2.   Number of lick in specific interval\n",
        "3.   pick pupil diameter\n",
        "4.   etc.\n",
        "\n",
        "References:\n",
        "\n",
        "\n",
        "*   [psyTrack](https://github.com/nicholas-roy/psytrack)\n",
        "*   [paper](<https://www.cell.com/neuron/fulltext/S0896-6273(20)30963-6?_returnURL=https%3A%2F%2Flinkinghub.elsevier.com%2Fretrieve%2Fpii%2FS0896627320309636%3Fshowall%3Dtrue>) and [colab](https://tinyurl.com/PsyTrack-colab)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QhY6nN-t0lTi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preliminary setup\n",
        "\n",
        "Libraries listed below will need to be install.\n",
        "In the original notebook the useage of oneibl requires `pip install ibllib==1.4.7` as later version are not working. Those libraries and code focus mostly on implementation on the Rumbaugh lab data for comparison with the original colab see [here](https://tinyurl.com/PsyTrack-colab)"
      ],
      "metadata": {
        "id": "bUcvebn-BE5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "from IPython.display import clear_output\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import glob\n",
        "import copy\n",
        "import sys\n",
        "import time\n",
        "\n",
        "# to use onelight library like in the paper need to have the following call\n",
        "!pip install ibllib==1.4.7\n",
        "from oneibl.onelight import ONE\n",
        "\n",
        "# Install then import PsyTrack\n",
        "!pip install psytrack==2.0\n",
        "import psytrack as psy\n",
        "\n",
        "# Set save path for all figures, decide whether to save permanently\n",
        "SPATH = \"Figures/\"\n",
        "!mkdir -p \"{SPATH}\"\n",
        "\n",
        "# Set matplotlib defaults for making files consistent in Illustrator\n",
        "colors = psy.COLORS\n",
        "zorder = psy.ZORDER\n",
        "plt.rcParams['figure.dpi'] = 140\n",
        "plt.rcParams['savefig.dpi'] = 300\n",
        "plt.rcParams['savefig.facecolor'] = (1,1,1,0)\n",
        "plt.rcParams['savefig.bbox'] = \"tight\"\n",
        "plt.rcParams['font.size'] = 10\n",
        "# plt.rcParams['font.family'] = 'sans-serif'     # not available in Colab\n",
        "# plt.rcParams['font.sans-serif'] = 'Helvetica'  # not available in Colab\n",
        "plt.rcParams['pdf.fonttype'] = 42\n",
        "plt.rcParams['xtick.labelsize'] = 10\n",
        "plt.rcParams['ytick.labelsize'] = 10\n",
        "plt.rcParams['axes.labelsize'] = 12\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "mnEVKaVJCbn0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f0b32cc6-b9f1-491c-bb20-a5f27744111b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ibllib==1.4.7\n",
            "  Downloading ibllib-1.4.7-py3-none-any.whl (204 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▋                              | 10 kB 18.5 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 20 kB 21.8 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 30 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 40 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |████████                        | 51 kB 12.1 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 61 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 71 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 81 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 92 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 102 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 112 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 122 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 133 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 143 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 153 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 163 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 174 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 184 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 194 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 204 kB 9.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from ibllib==1.4.7) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.16.4 in /usr/local/lib/python3.7/dist-packages (from ibllib==1.4.7) (1.19.5)\n",
            "Requirement already satisfied: click>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from ibllib==1.4.7) (7.1.2)\n",
            "Collecting mtscomp>=1.0.1\n",
            "  Downloading mtscomp-1.0.2-py2.py3-none-any.whl (16 kB)\n",
            "Collecting flake8>=3.7.8\n",
            "  Downloading flake8-4.0.1-py2.py3-none-any.whl (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 1.6 MB/s \n",
            "\u001b[?25hCollecting colorlog>=4.0.2\n",
            "  Downloading colorlog-6.6.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: opencv-python>=4.1.1.26 in /usr/local/lib/python3.7/dist-packages (from ibllib==1.4.7) (4.1.2.30)\n",
            "Collecting pynrrd>=0.4.0\n",
            "  Downloading pynrrd-0.4.2-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: seaborn>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from ibllib==1.4.7) (0.11.2)\n",
            "Collecting globus-sdk>=1.7.1\n",
            "  Downloading globus_sdk-3.3.1-py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 11.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.32.1 in /usr/local/lib/python3.7/dist-packages (from ibllib==1.4.7) (4.62.3)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.7/dist-packages (from ibllib==1.4.7) (2.23.0)\n",
            "Collecting phylib>=2.2\n",
            "  Downloading phylib-2.4.1-py2.py3-none-any.whl (96 kB)\n",
            "\u001b[K     |████████████████████████████████| 96 kB 3.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=3.0.3 in /usr/local/lib/python3.7/dist-packages (from ibllib==1.4.7) (3.2.2)\n",
            "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.7/dist-packages (from ibllib==1.4.7) (1.1.5)\n",
            "Collecting pyflakes<2.5.0,>=2.4.0\n",
            "  Downloading pyflakes-2.4.0-py2.py3-none-any.whl (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 2.5 MB/s \n",
            "\u001b[?25hCollecting pycodestyle<2.9.0,>=2.8.0\n",
            "  Downloading pycodestyle-2.8.0-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 432 kB/s \n",
            "\u001b[?25hCollecting mccabe<0.7.0,>=0.6.0\n",
            "  Downloading mccabe-0.6.1-py2.py3-none-any.whl (8.6 kB)\n",
            "Collecting importlib-metadata<4.3\n",
            "  Downloading importlib_metadata-4.2.0-py3-none-any.whl (16 kB)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/base_command.py\", line 180, in _main\n",
            "    status = self.run(options, args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/req_command.py\", line 199, in wrapper\n",
            "    return func(self, options, args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/commands/install.py\", line 319, in run\n",
            "    reqs, check_supported_wheels=not options.target_dir\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 128, in resolve\n",
            "    requirements, max_rounds=try_to_avoid_resolution_too_deep\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 473, in resolve\n",
            "    state = resolution.resolve(requirements, max_rounds=max_rounds)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 367, in resolve\n",
            "    failure_causes = self._attempt_to_pin_criterion(name)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 213, in _attempt_to_pin_criterion\n",
            "    criteria = self._get_criteria_to_update(candidate)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 203, in _get_criteria_to_update\n",
            "    name, crit = self._merge_into_criterion(r, parent=candidate)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 172, in _merge_into_criterion\n",
            "    if not criterion.candidates:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/resolvelib/structs.py\", line 139, in __bool__\n",
            "    return bool(self._sequence)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 143, in __bool__\n",
            "    return any(self)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 129, in <genexpr>\n",
            "    return (c for c in iterator if id(c) not in self._incompatible_ids)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 30, in _iter_built\n",
            "    for version, func in infos:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/resolution/resolvelib/factory.py\", line 272, in iter_index_candidate_infos\n",
            "    hashes=hashes,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/index/package_finder.py\", line 879, in find_best_candidate\n",
            "    candidates = self.find_all_candidates(project_name)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/index/package_finder.py\", line 824, in find_all_candidates\n",
            "    page_candidates = list(page_candidates_it)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/index/sources.py\", line 134, in page_candidates\n",
            "    yield from self._candidates_from_page(self._link)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/index/package_finder.py\", line 792, in process_project_url\n",
            "    links=page_links,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/index/package_finder.py\", line 772, in evaluate_links\n",
            "    candidate = self.get_install_candidate(link_evaluator, link)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/index/package_finder.py\", line 753, in get_install_candidate\n",
            "    is_candidate, result = link_evaluator.evaluate_link(link)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/index/package_finder.py\", line 186, in evaluate_link\n",
            "    if not wheel.supported(supported_tags):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/models/wheel.py\", line 95, in supported\n",
            "    return not self.file_tags.isdisjoint(tags)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/packaging/tags.py\", line 127, in __hash__\n",
            "    def __hash__(self):\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/main.py\", line 71, in main\n",
            "    return command.main(cmd_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/base_command.py\", line 104, in main\n",
            "    return self._main(args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/base_command.py\", line 213, in _main\n",
            "    logger.debug(\"Exception information:\", exc_info=True)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 1366, in debug\n",
            "    self._log(DEBUG, msg, args, **kwargs)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 1514, in _log\n",
            "    self.handle(record)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 1524, in handle\n",
            "    self.callHandlers(record)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 1586, in callHandlers\n",
            "    hdlr.handle(record)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 894, in handle\n",
            "    self.emit(record)\n",
            "  File \"/usr/lib/python3.7/logging/handlers.py\", line 71, in emit\n",
            "    logging.FileHandler.emit(self, record)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 1127, in emit\n",
            "    StreamHandler.emit(self, record)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 1025, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 869, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/utils/logging.py\", line 130, in format\n",
            "    formatted = super().format(record)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 616, in format\n",
            "    record.exc_text = self.formatException(record.exc_info)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 566, in formatException\n",
            "    traceback.print_exception(ei[0], ei[1], tb, None, sio)\n",
            "  File \"/usr/lib/python3.7/traceback.py\", line 104, in print_exception\n",
            "    type(value), value, tb, limit=limit).format(chain=chain):\n",
            "  File \"/usr/lib/python3.7/traceback.py\", line 508, in __init__\n",
            "    capture_locals=capture_locals)\n",
            "  File \"/usr/lib/python3.7/traceback.py\", line 363, in extract\n",
            "    f.line\n",
            "  File \"/usr/lib/python3.7/traceback.py\", line 285, in line\n",
            "    self._line = linecache.getline(self.filename, self.lineno).strip()\n",
            "  File \"/usr/lib/python3.7/linecache.py\", line 16, in getline\n",
            "    lines = getlines(filename, module_globals)\n",
            "  File \"/usr/lib/python3.7/linecache.py\", line 47, in getlines\n",
            "    return updatecache(filename, module_globals)\n",
            "  File \"/usr/lib/python3.7/linecache.py\", line 95, in updatecache\n",
            "    stat = os.stat(fullname)\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-3308ab97aa76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# to use onelight library like in the paper need to have the following call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install ibllib==1.4.7'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0moneibl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monelight\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mONE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Install then import PsyTrack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'oneibl'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieved published data\n",
        "The data from the paper are located on the MillerRumbaughLab shared drive to facilitate the access as some of the preprocessing to access those has been done ahead of time and will save loading and preprocessing time consuming steps.\n",
        "\n"
      ],
      "metadata": {
        "id": "uVfT1Jir6vI7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## from the IBL mouse data - figure 3\n",
        "ibl_data_path = tpath(r'Talks\\2022-01-13 - pillow\\Figures\\ibl-behavioral-data-Dec2019')\n",
        "ibl_mouse_data_path = pd.read_csv(ibl_data_path+os.sep+\"ibl_processed.csv\")\n",
        "MOUSE_DF = pd.read_csv(ibl_mouse_data_path)\n",
        "mID = 'CSHL_003'\n",
        "tmp = MOUSE_DF[MOUSE_DF['subject']=='CSHL_003'] \n",
        "# tmp.to_csv(ibl_data_path+os.sep+'CSHL_003'+\"_ibl_processed.csv\")\n",
        "\n",
        "\n",
        "## for the Rat DATA - figure 5 and after\n",
        "SPATH=  tpath(r'Talks\\2022-01-13 - pillow\\TestData')\n",
        "akrami_rat_data_path = tpath(r\"Talks\\2022-01-13 - pillow\\Brody_ratdata\\rat_behavior.csv\")\n",
        "RAT_DF = pd.read_csv(akrami_rat_data_path)\n",
        "RAT_DF = RAT_DF[RAT_DF[\"training_stage\"] > 2]  # Remove trials from early training\n",
        "RAT_DF = RAT_DF[~np.isnan(RAT_DF[\"choice\"])]   # Remove mistrials\n",
        "\n"
      ],
      "metadata": {
        "id": "ZlR8MQMC7PGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom functions"
      ],
      "metadata": {
        "id": "MVTpgUuvCsg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def listFiles(fpath):\n",
        "    pathoutput = fpath + os.sep + 'fileList.csv'\n",
        "    \n",
        "    if glob.glob(pathoutput) == [pathoutput]:\n",
        "        print('The files list has already been genereated:')\n",
        "        print(pathoutput)\n",
        "    else:\n",
        "\n",
        "        allFiles = set(glob.glob(fpath+'/**/*.xlsx', recursive=True)) # get all the excel file in folder\n",
        "        settingFiles = set(glob.glob(fpath+'/**/settings.xlsx', recursive = True)) # get all the settings file\n",
        "\n",
        "        wdilFiles = allFiles - settingFiles # exclude the settings files\n",
        "        wdilFiles = list(wdilFiles) # convert the set to a list\n",
        "\n",
        "        print('alllFiles: ', len(allFiles), ' wdilFiles: ', len(wdilFiles))\n",
        "        print(wdilFiles)\n",
        "\n",
        "        # important to sort the list to obtain proper sequence \n",
        "        wdilFiles.sort()\n",
        "        wdilFiles = pd.DataFrame({'file': wdilFiles})\n",
        "        wdilFiles.to_csv(pathoutput)\n",
        "\n",
        "        return print(wdilFiles)\n",
        "        return print('saved: ', pathoutput)\n",
        "\n",
        "def formatWDILfile(mypath, fromList = True):\n",
        "    '''\n",
        "    Function to concatenate all the wdil file into one \n",
        "    to be able to run psytrack\n",
        "\n",
        "    Args:\n",
        "    mypath (str): with all the files \n",
        "    fromList (logical): default to true this means that the files will be retrieved based on a list of folder not\n",
        "    just the excel file which are present \n",
        "    '''\n",
        "    if fromList == False:\n",
        "        allFiles = set(glob.glob(mypath+'/**/*.xlsx', recursive=True)) # get all the excel file in folder\n",
        "        settingFiles = set(glob.glob(mypath+'/**/settings.xlsx', recursive = True)) # get all the settings file\n",
        "\n",
        "        wdilFiles = allFiles - settingFiles # exclude the settings files\n",
        "        wdilFiles = list(wdilFiles) # convert the set to a list\n",
        "\n",
        "        # print('alllFiles: ', len(allFiles), ' wdilFiles: ', len(wdilFiles))\n",
        "        # print(wdilFiles)\n",
        "\n",
        "        # important to sort the list to obtain proper sequence \n",
        "        wdilFiles.sort()\n",
        "\n",
        "    else:\n",
        "        #####################################\n",
        "        ## TODO implement filter and category\n",
        "        #####################################\n",
        "\n",
        "        pathoutput = mypath + os.sep + 'fileList.csv'\n",
        "        wdilFiles = pd.read_csv(pathoutput)\n",
        "\n",
        "        if 'discard' in wdilFiles.columns:\n",
        "            wdilFiles = wdilFiles[wdilFiles['discard']!=1] # filter all the files that shouldnot be used\n",
        "\n",
        "        ## this section is to make it os invariant\n",
        "        if 'gvfs' in wdilFiles['file'][0] and sys.platform != 'linux':\n",
        "           wdilFiles['fileNoRoot'] = wdilFiles['file'].str.split('/run/user/1000/gvfs/smb-share:server=ishtar,share=millerrumbaughlab/').str[-1]\n",
        "           wdilFiles['file'] = wdilFiles.apply(lambda x: tpath(x['fileNoRoot']),axis = 1)\n",
        "\n",
        "        wdilFiles = list(wdilFiles['file'])\n",
        "\n",
        "        #####################################\n",
        "        ## TODO implement filter and category\n",
        "        #####################################\n",
        "\n",
        "    allDat = [] # create an empty object to \n",
        "    sID = []\n",
        "    \n",
        "    for i in wdilFiles:\n",
        "        print(i)\n",
        "        if '~' in i: # skip open files \n",
        "            continue\n",
        "        tmp = pd.read_excel(i) # read excel file\n",
        "\n",
        "        ## section to check with previous id and assign absolute order number\n",
        "        tmpsID = i.split(os.sep)[-3]\n",
        "        if tmpsID == sID:\n",
        "            # print('same')\n",
        "            k += 1\n",
        "        else:\n",
        "            k = 0\n",
        "        # print(k)\n",
        "\n",
        "\n",
        "        sID = i.split(os.sep)[-3] # add a column with the id\n",
        "        sessionDate = i.split(os.sep)[-2]\n",
        "        tmp['sID'] = sID\n",
        "        tmp['sessionDate'] = sessionDate\n",
        "        tmp['session'] = k\n",
        "        # absSession = # obtain the absolute session number\n",
        "\n",
        "        allDat.append(tmp)\n",
        "    allDat = pd.concat(allDat)\n",
        "\n",
        "    return allDat\n",
        "\n",
        "def codingDatFile(allDat):\n",
        "    '''\n",
        "    Function to code all the wdil file into one \n",
        "    to be able to run psytrack\n",
        "\n",
        "    Args:\n",
        "    allDat\n",
        "    '''\n",
        "\n",
        "    allDat['choice'] = allDat['Lick?']\n",
        "    allDat = allDat.rename(columns={'Trial#':'trial'})\n",
        "    ## establish what are the hit \n",
        "    ## in this case the establishment of hit correspond to true hit:\n",
        "    ##      lick and was a Go # if Go=1 and Correct =1 --> CorrectCat ==2 and is a hit\n",
        "    ## as well as correct rejection:\n",
        "    ##      with held and was a no Go # if Go=0 and Correct =0 --> CorrectCat ==0 and is a hit\n",
        "    allDat['CorrectCat']  = allDat['choice'] + allDat['Correct?'] \n",
        "    allDat['hit'] = np.where((allDat['CorrectCat']==2) | (allDat['CorrectCat']==0),1,0)\n",
        "\n",
        "    ## those could be useful for modeling see\n",
        "    ## comments on Figure F3b use of bias\n",
        "    allDat['Go'] = allDat['Go/NoGo']\n",
        "    allDat['NoGo'] = abs(allDat['Go/NoGo']-1)\n",
        "\n",
        "    return allDat\n",
        "\n",
        "def getRat(subject, first=20000, cutoff=50):\n",
        "\n",
        "    df = RAT_DF[RAT_DF['subject_id']==subject]  # restrict dataset to single subject\n",
        "    df = df[:first]  # restrict to \"first\" trials of data\n",
        "    # remove sessions with fewer than \"cutoff\" valid trials\n",
        "    df = df.groupby('session').filter(lambda x: len(x) >= cutoff)   \n",
        "\n",
        "    # Normalize the stimuli to standard normal\n",
        "    s_a = (df[\"s_a\"] - np.mean(df[\"s_a\"]))/np.std(df[\"s_a\"])\n",
        "    s_b = (df[\"s_b\"] - np.mean(df[\"s_b\"]))/np.std(df[\"s_b\"])\n",
        "    \n",
        "    # Determine which trials do not have a valid previous trial (mistrial or session boundary)\n",
        "    t = np.array(df[\"trial\"])\n",
        "    prior = ((t[1:] - t[:-1]) == 1).astype(int)\n",
        "    prior = np.hstack(([0], prior))\n",
        "\n",
        "    # Calculate previous average tone value\n",
        "    s_avg = (df[\"s_a\"][:-1] + df[\"s_b\"][:-1])/2\n",
        "    s_avg = (s_avg - np.mean(s_avg))/np.std(s_avg)\n",
        "    s_avg = np.hstack(([0], s_avg))\n",
        "    s_avg = s_avg * prior  # for trials without a valid previous trial, set to 0\n",
        "\n",
        "    # Calculate previous correct answer\n",
        "    h = (df[\"correct_side\"][:-1] * 2 - 1).astype(int)   # map from (0,1) to (-1,1)\n",
        "    h = np.hstack(([0], h))\n",
        "    h = h * prior  # for trials without a valid previous trial, set to 0\n",
        "    \n",
        "    # Calculate previous choice\n",
        "    c = (df[\"choice\"][:-1] * 2 - 1).astype(int)   # map from (0,1) to (-1,1)\n",
        "    c = np.hstack(([0], c))\n",
        "    c = c * prior  # for trials without a valid previous trial, set to 0\n",
        "    \n",
        "    inputs = dict(s_a = np.array(s_a)[:, None],\n",
        "                  s_b = np.array(s_b)[:, None],\n",
        "                  s_avg = np.array(s_avg)[:, None],\n",
        "                  h = np.array(h)[:, None],\n",
        "                  c = np.array(c)[:, None])\n",
        "\n",
        "    dat = dict(\n",
        "        subject = subject,\n",
        "        inputs = inputs,\n",
        "        s_a = np.array(df['s_a']),\n",
        "        s_b = np.array(df['s_b']),\n",
        "        correct = np.array(df['hit']),\n",
        "        answer = np.array(df['correct_side']),\n",
        "        y = np.array(df['choice']),\n",
        "        dayLength=np.array(df.groupby(['session']).size()),\n",
        "    )\n",
        "    return dat\n",
        "\n",
        "def getMouse(subject, p=5):\n",
        "    df = MOUSE_DF[MOUSE_DF['subject']==subject]   # Restrict data to the subject specified\n",
        "    \n",
        "    cL = np.tanh(p*df['contrastLeft'])/np.tanh(p)   # tanh transformation of left contrasts\n",
        "    cR = np.tanh(p*df['contrastRight'])/np.tanh(p)  # tanh transformation of right contrasts\n",
        "    inputs = dict(cL = np.array(cL)[:, None], cR = np.array(cR)[:, None])\n",
        "\n",
        "    dat = dict(\n",
        "        subject=subject,\n",
        "        lab=np.unique(df[\"lab\"])[0],\n",
        "        contrastLeft=np.array(df['contrastLeft']),\n",
        "        contrastRight=np.array(df['contrastRight']),\n",
        "        date=np.array(df['date']),\n",
        "        dayLength=np.array(df.groupby(['date','session']).size()),\n",
        "        correct=np.array(df['feedbackType']),\n",
        "        answer=np.array(df['answer']),\n",
        "        probL=np.array(df['probabilityLeft']),\n",
        "        inputs = inputs,\n",
        "        y = np.array(df['choice'])\n",
        "    )\n",
        "    \n",
        "    return dat\n",
        "\n",
        "def convertToDictRat(allDat, subject, first=20000, cutoff=50):\n",
        "\n",
        "    '''\n",
        "    equivalent to the function getRat from the paper see above and here https://tinyurl.com/PsyTrack-colab\n",
        "    '''\n",
        "\n",
        "    df = allDat[allDat['sID']==subject]  # restrict dataset to single subject\n",
        "    df = df[:first] # restrict to \"first\" trials of data\n",
        "    # # remove sessions with fewer than \"cutoff\" valid trials\n",
        "    # df = df.groupby('session').filter(lambda x: len(x) >= cutoff)   \n",
        "\n",
        "    # Determine which trials do not have a valid previous trial (mistrial or session boundary)\n",
        "    t = np.array(df[\"trial\"])\n",
        "    prior = ((t[1:] - t[:-1]) == 1).astype(int)\n",
        "    prior = np.hstack(([0], prior))\n",
        "\n",
        "    # Calculate previous correct answer\n",
        "    h = (df[\"Correct?\"][:-1] * 2 - 1).astype(int)   # map from (0,1) to (-1,1)\n",
        "    h = np.hstack(([0], h))\n",
        "    h = h * prior  # for trials without a valid previous trial, set to 0\n",
        "    \n",
        "    # Calculate previous choice\n",
        "    c = (df[\"choice\"][:-1] * 2 - 1).astype(int)   # map from (0,1) to (-1,1)\n",
        "    c = np.hstack(([0], c))\n",
        "    c = c * prior  # for trials without a valid previous trial, set to 0\n",
        "    \n",
        "    ## note here that it could be useful to have different stimulus values \n",
        "    ## important to respect the dictionary psy.COLORS hence the name of the specific names of the inputs\n",
        "    inputs = dict(s1 = np.array(df['Go/NoGo'])[:, None], \n",
        "                  h = np.array(h)[:, None],\n",
        "                  c = np.array(c)[:, None])\n",
        "\n",
        "    dat = dict(\n",
        "        subject = subject,\n",
        "        inputs = inputs,\n",
        "        s1 = np.array(df['Go/NoGo']), # correspond to the go/noGo stim\n",
        "        correct = np.array(df['hit']), # hit correspond to hit and correct rejection\n",
        "        answer = np.array(df['Correct?']), #this is the answer \n",
        "        y = np.array(df['choice']), #this correspond to the Lick\n",
        "        dayLength=np.array(df.groupby(['session']).size()),\n",
        "    )\n",
        "\n",
        "    return dat\n",
        "\n",
        "def convertToDictMouse(allDat, subject, first=20000, cutoff=50):\n",
        "\n",
        "    '''\n",
        "    equivalent to the function getRat from the paper see above and here https://tinyurl.com/PsyTrack-colab\n",
        "    '''\n",
        "\n",
        "    df = allDat[allDat['sID']==subject]  # restrict dataset to single subject\n",
        "    df = df[:first] # restrict to \"first\" trials of data\n",
        "    # # remove sessions with fewer than \"cutoff\" valid trials\n",
        "    # df = df.groupby('session').filter(lambda x: len(x) >= cutoff)   \n",
        "\n",
        "    # Determine which trials do not have a valid previous trial (mistrial or session boundary)\n",
        "    t = np.array(df[\"trial\"])\n",
        "    prior = ((t[1:] - t[:-1]) == 1).astype(int)\n",
        "    prior = np.hstack(([0], prior))\n",
        "\n",
        "    # Calculate previous correct answer\n",
        "    h = (df[\"Correct?\"][:-1] * 2 - 1).astype(int)   # map from (0,1) to (-1,1)\n",
        "    h = np.hstack(([0], h))\n",
        "    h = h * prior  # for trials without a valid previous trial, set to 0\n",
        "    \n",
        "    # Calculate previous choice\n",
        "    c = (df[\"choice\"][:-1] * 2 - 1).astype(int)   # map from (0,1) to (-1,1)\n",
        "    c = np.hstack(([0], c))\n",
        "    c = c * prior  # for trials without a valid previous trial, set to 0\n",
        "    \n",
        "    # note here that it could be useful to have different stimulus values \n",
        "    inputs = dict(stim = np.array(df['Go/NoGo'])[:, None], \n",
        "                  h = np.array(h)[:, None],\n",
        "                  c = np.array(c)[:, None])\n",
        "\n",
        "    dat = dict(\n",
        "        subject = subject,\n",
        "        inputs = inputs,\n",
        "        stim = np.array(df['Go/NoGo']), # correspond to the go/noGo stim\n",
        "        correct = np.array(df['hit']), # hit correspond to hit and correct rejection\n",
        "        answer = np.array(df['Correct?']), #this is the answer \n",
        "        y = np.array(df['choice']), #this correspond to the Lick\n",
        "        dayLength=np.array(df.groupby(['session']).size()),\n",
        "    )\n",
        "\n",
        "    return dat\n",
        "\n",
        "def tpath(mypath, shareDrive = 'Y'):\n",
        "    '''\n",
        "    path conversion to switch form linux to windows platform with define drive\n",
        "    Args:\n",
        "    mypath (str): path of the file of interest\n",
        "    shareDrive (str): windows letter of the shared folder\n",
        "    '''\n",
        "    if ('google.colab' in str(get_ipython())) or sys.platform == 'win32':\n",
        "         myRoot = shareDrive+':'      \n",
        "    else:\n",
        "        myRoot = '/run/user/1000/gvfs/smb-share:server=ishtar,share=millerrumbaughlab'\n",
        "\n",
        "\n",
        "    newpath = myRoot+os.sep+mypath\n",
        "\n",
        "    return newpath\n",
        "\n",
        "def psyCompute(allDat, SPATH, sID, figure_off = False):\n",
        "    fname = SPATH+os.sep+str(sID)+'_fig5b_data.npz'\n",
        "\n",
        "    ## either load or generate the data\n",
        "    if glob.glob(fname) == [fname]:\n",
        "        dat = np.load(fname, allow_pickle=True)['dat'].item()\n",
        "    else:\n",
        "        ## convert the data\n",
        "        outData = convertToDictRat(allDat, sID)\n",
        "        new_dat = psy.trim(outData, START=0, END=12500)\n",
        "\n",
        "        # here weights could be adjusted \n",
        "        weights = {'bias': 1, 's1': 1, 'h': 1, 'c': 1}\n",
        "        K = np.sum([weights[i] for i in weights.keys()])\n",
        "        # hyper guess are kept with default value as in the paper\n",
        "        hyper_guess = {\n",
        "         'sigma'   : [2**-5]*K,\n",
        "         'sigInit' : 2**5,\n",
        "         'sigDay'  : [2**-4]*K,\n",
        "          }\n",
        "        optList = ['sigma', 'sigDay']\n",
        "\n",
        "        hyp, evd, wMode, hess_info = psy.hyperOpt(new_dat, hyper_guess, weights, optList)\n",
        "\n",
        "        dat = {'hyp' : hyp, 'evd' : evd, 'wMode' : wMode, 'W_std' : hess_info['W_std'],\n",
        "               'weights' : weights, 'new_dat' : new_dat}\n",
        "\n",
        "        # Save interim result\n",
        "        np.savez_compressed(SPATH+os.sep+str(sID)+'_fig5b_data.npz', dat=dat)\n",
        "\n",
        "        if figure_off == True:\n",
        "            # save the figure\n",
        "            fig = psy.plot_weights(dat['wMode'], dat['weights'], days=dat['new_dat'][\"dayLength\"], \n",
        "                                   errorbar=dat['W_std'], figsize=(4.75,1.4))\n",
        "            # plt.xlabel(None); plt.ylabel(None)\n",
        "            # plt.subplots_adjust(0,0,1,1) \n",
        "            plt.savefig(SPATH +os.sep+str(sID)+ \"Fig5b.pdf\")\n",
        "\n",
        "\n",
        "def plot_all(all_labels, all_w, Weights, figsize):\n",
        "    fig = plt.figure(figsize=figsize)\n",
        "    Weights = [Weights] if type(Weights) is str else Weights\n",
        "    avg_len=6000 # this needs to be truncated for the average for the array to have same dimensions\n",
        "    for i, W in enumerate(Weights):\n",
        "        print(i,W)\n",
        "        avg = []\n",
        "        for i in np.arange(0,len(all_w),1):\n",
        "            print(i)\n",
        "            bias_ind = np.where(all_labels[i] == W)[0][-1]\n",
        "            bias_w = all_w[i][bias_ind]\n",
        "            avg += [list(bias_w[:avg_len]) + [np.nan]*(avg_len - len(bias_w[:avg_len]))]\n",
        "            colors = psy.COLORS\n",
        "            plt.plot(bias_w, color=colors[W], alpha=0.2, lw=1, zorder=2+i)\n",
        "        plt.plot(np.nanmean(avg, axis=0), color=colors[W], alpha=0.8, lw=2.5, zorder=5+i)\n",
        "\n",
        "    plt.axhline(0, color=\"black\", linestyle=\"--\", lw=1, alpha=0.5, zorder=1)\n",
        "    plt.tight_layout()\n",
        "    # plt.gca().spines['right'].set_visible(False)\n",
        "    # plt.gca().spines['top'].set_visible(False)\n",
        "    # plt.xlim(0, 6000)\n",
        "    # plt.ylim(-2.5, 2.5)\n",
        "    return fig\n",
        "\n",
        "def plotLabelsandW(geno, SPATH):\n",
        "    '''\n",
        "    this function will output all the labels and wheight for a givien genotype\n",
        "    based on the corresponding files and select the genotype of interest\n",
        "\n",
        "    args:\n",
        "    corresp(pd.DataFrame): data frame with file path sID and geno\n",
        "    geno(str): geno of intrest either 'wt' or 'het'\n",
        "    '''\n",
        "    \n",
        "    ## have the corresponding file generatede \n",
        "    npzFiles = glob.glob(SPATH+os.sep+'*.npz')\n",
        "    corresp = pd.DataFrame({'fname':npzFiles})\n",
        "    corresp['sID'] = corresp['fname'].str.split(os.sep).str[-1].str.split('_').str[0].astype(int)\n",
        "   \n",
        "    ## check and implement the genotypes\n",
        "    try: geno\n",
        "    except: geno = None\n",
        "    if geno is None:\n",
        "        geno = pd.read_csv(os.sep.join(SPATH.split(os.sep)[:-2])+os.sep+'animals.csv')\n",
        "\n",
        "    corresp = pd.merge(corresp, geno, on='sID')\n",
        "\n",
        "    ## \n",
        "    cWTorHet = corresp[corresp['geno']=='wt']\n",
        "    \n",
        "    all_labels = []\n",
        "    all_w = []\n",
        "\n",
        "    for i,j in cWTorHet.iterrows():\n",
        "        print(i,j)\n",
        "        rat = np.load(j['fname'], allow_pickle=True)['dat'].item()\n",
        "        \n",
        "        labels = []\n",
        "        for j in sorted(rat['weights'].keys()):\n",
        "            labels += [j]*rat['weights'][j]\n",
        "            \n",
        "        all_labels += [np.array(labels)]\n",
        "        all_w += [rat['wMode']] \n",
        "\n",
        "\n",
        "    myFigsize = (3.6,1.8)\n",
        "    plot_all(all_labels, all_w, [\"s1\"], myFigsize)\n",
        "    plt.ylim(-1, 15)\n",
        "    # plt.subplots_adjust(0,0,1,1) \n",
        "    # plt.gca().set_yticks([-2,0,2])\n",
        "    # plt.gca().set_xticklabels([])\n",
        "    plt.savefig(SPATH +os.sep+ geno+ \"Fig6a.pdf\")\n",
        "\n",
        "    plot_all(all_labels, all_w, [\"bias\"], myFigsize)\n",
        "    plt.ylim(-7, 2)\n",
        "    # plt.gca().set_yticks([-2,0,2])\n",
        "    # plt.gca().set_xticklabels([])\n",
        "    # plt.gca().set_yticklabels([])\n",
        "    # plt.subplots_adjust(0,0,1,1) \n",
        "    plt.savefig(SPATH +os.sep+ geno+ \"Fig6b.pdf\")\n",
        "\n",
        "    plot_all(all_labels, all_w, [\"h\"], myFigsize)\n",
        "    plt.ylim(-1, 1)\n",
        "    # # plt.gca().set_yticklabels([])\n",
        "    # plt.subplots_adjust(0,0,1,1) \n",
        "    plt.savefig(SPATH +os.sep+ geno+ \"Fig6d.pdf\")\n",
        "\n",
        "\n",
        "    plot_all(all_labels, all_w, [\"c\"], myFigsize)\n",
        "    plt.ylim(-1, 1)\n",
        "    # plt.gca().set_yticklabels([])\n",
        "    # plt.gca().set_xticklabels([])\n",
        "    # plt.subplots_adjust(0,0,1,1) \n",
        "    plt.savefig(SPATH +os.sep+ geno+ \"Fig6e.pdf\")"
      ],
      "metadata": {
        "id": "yzI1Ew69CwpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WDIL data retrieval"
      ],
      "metadata": {
        "id": "sA4RZwl9CzoF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) generate a list of files for the experiment\n",
        "\n",
        "This list can then be edited while keeping the original data and will:\n",
        "\n",
        "\n",
        "*   enable coding of missing data\n",
        "*   add detail on experimental parameters of stimulation\n",
        "  * phase (code for specific stimulus protocol to define)\n",
        "  * lick rate\n",
        "  * peak pupil\n",
        "\n",
        "Also need:\n",
        "\n",
        "\n",
        "*   list of genotypes named `animals.csv`\n",
        "*   list of what animals met criteria \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xdx092UjGet9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cohorts = [tpath(r'Sheldon\\All_WDIL\\WDIL009_EMXcreRUM2_7-20-21\\WDIL009_forpsytrack'),  tpath(r'Sheldon\\All_WDIL\\for psytrack\\WDIL007Box1+2'), tpath(r'Sheldon\\All_WDIL\\for psytrack\\WDIL010Box1+2')] # list of all the path and cohort of interest\n",
        "# mypath = cohorts[1]/\n",
        "\n",
        "### create and check for the full file list\n",
        "for mypath in cohorts:\n",
        "    print(mypath)\n",
        "    SPATH =  mypath+os.sep+'output'\n",
        "    os.makedirs(SPATH, exist_ok = True)\n",
        "\n",
        "    ## create a list of files \n",
        "    listFiles(mypath)\n",
        "\n",
        "    ## load or generate the data\n",
        "    # if glob.glob(mypath+os.sep+'allDat.csv') == [mypath+os.sep+'allDat.csv']:\n",
        "    #     allDat = pd.read_csv(mypath+os.sep+'allDat.csv')\n",
        "    # else:\n",
        "    #     allDat = formatWDILfile(mypath)\n",
        "    #     allDat.to_csv(mypath+os.sep+'allDat.csv')\n",
        "\n"
      ],
      "metadata": {
        "id": "zfkSC77vhfxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2) get data from csv file list"
      ],
      "metadata": {
        "id": "m-El4uzoKr_4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part is important and deal with missing data. In practice it is better to drop the data than to replace them in this specific context.\n"
      ],
      "metadata": {
        "id": "tn4E4rAmi53C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## coding dat file\n",
        "mypath = cohorts[1]\n",
        "SPATH =  mypath+os.sep+'output'\n",
        "if glob.glob(mypath+os.sep+'allDat.csv') == [mypath+os.sep+'allDat.csv']:\n",
        "    allDat = pd.read_csv(mypath+os.sep+'allDat.csv')\n",
        "else:\n",
        "    allDat = formatWDILfile(mypath)\n",
        "    allDat.to_csv(mypath+os.sep+'allDat.csv')"
      ],
      "metadata": {
        "id": "Yi1wjD40JDBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Dealing with NaN\n",
        "## important dispaly item for double check\n",
        "## dealing with potential missing data in the folder\n",
        "print(allDat[allDat.isnull().any(axis=1)])\n",
        "allDat = allDat.dropna()"
      ],
      "metadata": {
        "id": "nfY4G7lljB_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Coding of the file in a proper format for further processing"
      ],
      "metadata": {
        "id": "HkdPWXW3jQAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "allDat = codingDatFile(allDat)\n",
        "## add genotype info to the file\n",
        "## adding the genotype to the dat file\n",
        "## then as a first pass can split the file and process wt or het\n",
        "geno = pd.read_csv(mypath+os.sep+\"animals.csv\")\n",
        "allDat = pd.merge(allDat, geno, on ='sID')"
      ],
      "metadata": {
        "id": "SN7xdIFQjQkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test data WDIL007 sID 1753"
      ],
      "metadata": {
        "id": "3z_FGuY2pcfR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sID = 1753 #input the idname of the subeject\n",
        "fname = SPATH+os.sep+str(sID)+'_fig5b_data.npz'\n",
        "psyCompute(allDat, sID)"
      ],
      "metadata": {
        "id": "G847i0J2yqqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test data all WDIL007 - corresponding to Fig6"
      ],
      "metadata": {
        "id": "Zb-5bPz3yg-s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) generate all the model by individual\n",
        "*takes roughly 40 min for 12 animals in our data sets on 6 core CPU. It depends a lot of th enumber of trials and parameters to model*"
      ],
      "metadata": {
        "id": "DV3FEGnKJNsg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## takes roughly 30 min with 6 core CPU\n",
        "## length highly dependent on number of trials etc.\n",
        "all_id = allDat['sID'].unique()\n",
        "a = time.time()\n",
        "for i, sID in enumerate(all_id):\n",
        "    print(i, sID)\n",
        "    try:\n",
        "        psyCompute(allDat, SPATH, sID) ## psyCompute is already parallelized on CPU thus the more cpu the better\n",
        "    except:\n",
        "        print('error with: ', sID)\n",
        "b = time.time()\n",
        "print(b-a)"
      ],
      "metadata": {
        "id": "xUyNVM68ynUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) generate graphs base on specific criteria"
      ],
      "metadata": {
        "id": "xJT4k-4tJUwN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in ['wt', 'het']:\n",
        "    plotLabelsandW(geno=i, SPATH)\n"
      ],
      "metadata": {
        "id": "npvAbn0aj45e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}