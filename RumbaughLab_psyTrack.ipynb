{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RumbaughLab_psyTrack.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "bUcvebn-BE5h",
        "MVTpgUuvCsg7"
      ],
      "authorship_tag": "ABX9TyMiKH7M6GsloGL4Ptcjd5Ih",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RumbaughLab/colab/blob/main/RumbaughLab_psyTrack.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# _Implementation of psyTrack for Rumbaugh Lab_\n",
        "## Figure Generator\n",
        "\n",
        "relevant to:\n",
        "1. Cris Creson\n",
        "2. Sheldon Michaelson\n",
        "3. Randy Golvin\n",
        "4. Tom Vaissiere\n",
        "\n",
        "\n",
        "_(v1.0, last updated February, 02, 2022 - 4:47 PM)_\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "3cl2R3bwztnV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook will attempt at the implementation of psyTrack on WDIL data and especially the WDIL dataset WDIL0007 which are located in a shared folder.\n",
        "Although this has been edited in colab it is not meant to be use in colab but rather to be run locally as a jupyter notebook as the data can be accessed through Scripps Network without long upload time.\n",
        "\n",
        "Important note: modification of the inuputs should enable analysis of other WDIL dataset\n",
        "\n",
        "Several additional layres can be incorporated like:\n",
        "\n",
        "\n",
        "1.   Stimulus intensities\n",
        "2.   Number of lick in specific interval\n",
        "3.   pick pupil diameter\n",
        "4.   etc.\n",
        "\n",
        "References:\n",
        "\n",
        "\n",
        "*   [psyTrack](https://github.com/nicholas-roy/psytrack)\n",
        "*   [paper](<https://www.cell.com/neuron/fulltext/S0896-6273(20)30963-6?_returnURL=https%3A%2F%2Flinkinghub.elsevier.com%2Fretrieve%2Fpii%2FS0896627320309636%3Fshowall%3Dtrue>) and [colab](https://tinyurl.com/PsyTrack-colab)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QhY6nN-t0lTi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preliminary setup\n",
        "\n",
        "Libraries listed below will need to be install.\n",
        "In the original notebook the useage of oneibl requires `pip install ibllib==1.4.7` as later version are not working. Those libraries and code focus mostly on implementation on the Rumbaugh lab data for comparison with the original colab see [here](https://tinyurl.com/PsyTrack-colab)"
      ],
      "metadata": {
        "id": "bUcvebn-BE5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "from IPython.display import clear_output\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import glob\n",
        "import copy\n",
        "import sys\n",
        "import time\n",
        "\n",
        "# to use onelight library like in the paper need to have the following call\n",
        "!pip install ibllib==1.4.7\n",
        "from oneibl.onelight import ONE\n",
        "\n",
        "# Install then import PsyTrack\n",
        "!pip install psytrack==2.0\n",
        "import psytrack as psy\n",
        "\n",
        "# Set save path for all figures, decide whether to save permanently\n",
        "SPATH = \"Figures/\"\n",
        "!mkdir -p \"{SPATH}\"\n",
        "\n",
        "# Set matplotlib defaults for making files consistent in Illustrator\n",
        "colors = psy.COLORS\n",
        "zorder = psy.ZORDER\n",
        "plt.rcParams['figure.dpi'] = 140\n",
        "plt.rcParams['savefig.dpi'] = 300\n",
        "plt.rcParams['savefig.facecolor'] = (1,1,1,0)\n",
        "plt.rcParams['savefig.bbox'] = \"tight\"\n",
        "plt.rcParams['font.size'] = 10\n",
        "# plt.rcParams['font.family'] = 'sans-serif'     # not available in Colab\n",
        "# plt.rcParams['font.sans-serif'] = 'Helvetica'  # not available in Colab\n",
        "plt.rcParams['pdf.fonttype'] = 42\n",
        "plt.rcParams['xtick.labelsize'] = 10\n",
        "plt.rcParams['ytick.labelsize'] = 10\n",
        "plt.rcParams['axes.labelsize'] = 12\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "mnEVKaVJCbn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieved published data\n",
        "The data from the paper are located on the MillerRumbaughLab shared drive to facilitate the access as some of the preprocessing to access those has been done ahead of time and will save loading and preprocessing time consuming steps.\n",
        "\n"
      ],
      "metadata": {
        "id": "uVfT1Jir6vI7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## from the IBL mouse data - figure 3\n",
        "ibl_data_path = tpath(r'Talks\\2022-01-13 - pillow\\Figures\\ibl-behavioral-data-Dec2019')\n",
        "ibl_mouse_data_path = pd.read_csv(ibl_data_path+os.sep+\"ibl_processed.csv\")\n",
        "MOUSE_DF = pd.read_csv(ibl_mouse_data_path)\n",
        "mID = 'CSHL_003'\n",
        "tmp = MOUSE_DF[MOUSE_DF['subject']=='CSHL_003'] \n",
        "# tmp.to_csv(ibl_data_path+os.sep+'CSHL_003'+\"_ibl_processed.csv\")\n",
        "\n",
        "\n",
        "## for the Rat DATA - figure 5 and after\n",
        "SPATH=  tpath(r'Talks\\2022-01-13 - pillow\\TestData')\n",
        "akrami_rat_data_path = tpath(r\"Talks\\2022-01-13 - pillow\\Brody_ratdata\\rat_behavior.csv\")\n",
        "RAT_DF = pd.read_csv(akrami_rat_data_path)\n",
        "RAT_DF = RAT_DF[RAT_DF[\"training_stage\"] > 2]  # Remove trials from early training\n",
        "RAT_DF = RAT_DF[~np.isnan(RAT_DF[\"choice\"])]   # Remove mistrials\n",
        "\n"
      ],
      "metadata": {
        "id": "ZlR8MQMC7PGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom functions"
      ],
      "metadata": {
        "id": "MVTpgUuvCsg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def listFiles(fpath):\n",
        "    pathoutput = fpath + os.sep + 'fileList.csv'\n",
        "    \n",
        "    if glob.glob(pathoutput) == [pathoutput]:\n",
        "        print('The files list has already been genereated:')\n",
        "        print(pathoutput)\n",
        "    else:\n",
        "\n",
        "        allFiles = set(glob.glob(fpath+'/**/*.xlsx', recursive=True)) # get all the excel file in folder\n",
        "        settingFiles = set(glob.glob(fpath+'/**/settings.xlsx', recursive = True)) # get all the settings file\n",
        "\n",
        "        wdilFiles = allFiles - settingFiles # exclude the settings files\n",
        "        wdilFiles = list(wdilFiles) # convert the set to a list\n",
        "\n",
        "        print('alllFiles: ', len(allFiles), ' wdilFiles: ', len(wdilFiles))\n",
        "        print(wdilFiles)\n",
        "\n",
        "        # important to sort the list to obtain proper sequence \n",
        "        wdilFiles.sort()\n",
        "        wdilFiles = pd.DataFrame({'file': wdilFiles})\n",
        "        wdilFiles.to_csv(pathoutput)\n",
        "\n",
        "        return print(wdilFiles)\n",
        "        return print('saved: ', pathoutput)\n",
        "\n",
        "def formatWDILfile(mypath, fromList = True):\n",
        "    '''\n",
        "    Function to concatenate all the wdil file into one \n",
        "    to be able to run psytrack\n",
        "\n",
        "    Args:\n",
        "    mypath (str): with all the files \n",
        "    fromList (logical): default to true this means that the files will be retrieved based on a list of folder not\n",
        "    just the excel file which are present \n",
        "    '''\n",
        "    if fromList == False:\n",
        "        allFiles = set(glob.glob(mypath+'/**/*.xlsx', recursive=True)) # get all the excel file in folder\n",
        "        settingFiles = set(glob.glob(mypath+'/**/settings.xlsx', recursive = True)) # get all the settings file\n",
        "\n",
        "        wdilFiles = allFiles - settingFiles # exclude the settings files\n",
        "        wdilFiles = list(wdilFiles) # convert the set to a list\n",
        "\n",
        "        # print('alllFiles: ', len(allFiles), ' wdilFiles: ', len(wdilFiles))\n",
        "        # print(wdilFiles)\n",
        "\n",
        "        # important to sort the list to obtain proper sequence \n",
        "        wdilFiles.sort()\n",
        "\n",
        "    else:\n",
        "        #####################################\n",
        "        ## TODO implement filter and category\n",
        "        #####################################\n",
        "\n",
        "        pathoutput = mypath + os.sep + 'fileList.csv'\n",
        "        wdilFiles = pd.read_csv(pathoutput)\n",
        "\n",
        "        if 'discard' in wdilFiles.columns:\n",
        "            wdilFiles = wdilFiles[wdilFiles['discard']!=1] # filter all the files that shouldnot be used\n",
        "\n",
        "        ## this section is to make it os invariant\n",
        "        if 'gvfs' in wdilFiles['file'][0] and sys.platform != 'linux':\n",
        "           wdilFiles['fileNoRoot'] = wdilFiles['file'].str.split('/run/user/1000/gvfs/smb-share:server=ishtar,share=millerrumbaughlab/').str[-1]\n",
        "           wdilFiles['file'] = wdilFiles.apply(lambda x: tpath(x['fileNoRoot']),axis = 1)\n",
        "\n",
        "        wdilFiles = list(wdilFiles['file'])\n",
        "\n",
        "        #####################################\n",
        "        ## TODO implement filter and category\n",
        "        #####################################\n",
        "\n",
        "    allDat = [] # create an empty object to \n",
        "    sID = []\n",
        "    \n",
        "    for i in wdilFiles:\n",
        "        print(i)\n",
        "        if '~' in i: # skip open files \n",
        "            continue\n",
        "        tmp = pd.read_excel(i) # read excel file\n",
        "\n",
        "        ## section to check with previous id and assign absolute order number\n",
        "        tmpsID = i.split(os.sep)[-3]\n",
        "        if tmpsID == sID:\n",
        "            # print('same')\n",
        "            k += 1\n",
        "        else:\n",
        "            k = 0\n",
        "        # print(k)\n",
        "\n",
        "\n",
        "        sID = i.split(os.sep)[-3] # add a column with the id\n",
        "        sessionDate = i.split(os.sep)[-2]\n",
        "        tmp['sID'] = sID\n",
        "        tmp['sessionDate'] = sessionDate\n",
        "        tmp['session'] = k\n",
        "        # absSession = # obtain the absolute session number\n",
        "\n",
        "        allDat.append(tmp)\n",
        "    allDat = pd.concat(allDat)\n",
        "\n",
        "    return allDat\n",
        "\n",
        "def codingDatFile(allDat):\n",
        "    '''\n",
        "    Function to code all the wdil file into one \n",
        "    to be able to run psytrack\n",
        "\n",
        "    Args:\n",
        "    allDat\n",
        "    '''\n",
        "\n",
        "    allDat['choice'] = allDat['Lick?']\n",
        "    allDat = allDat.rename(columns={'Trial#':'trial'})\n",
        "    ## establish what are the hit \n",
        "    ## in this case the establishment of hit correspond to true hit:\n",
        "    ##      lick and was a Go # if Go=1 and Correct =1 --> CorrectCat ==2 and is a hit\n",
        "    ## as well as correct rejection:\n",
        "    ##      with held and was a no Go # if Go=0 and Correct =0 --> CorrectCat ==0 and is a hit\n",
        "    allDat['CorrectCat']  = allDat['choice'] + allDat['Correct?'] \n",
        "    allDat['hit'] = np.where((allDat['CorrectCat']==2) | (allDat['CorrectCat']==0),1,0)\n",
        "\n",
        "    ## those could be useful for modeling see\n",
        "    ## comments on Figure F3b use of bias\n",
        "    allDat['Go'] = allDat['Go/NoGo']\n",
        "    allDat['NoGo'] = abs(allDat['Go/NoGo']-1)\n",
        "\n",
        "    return allDat\n",
        "\n",
        "def getRat(subject, first=20000, cutoff=50):\n",
        "\n",
        "    df = RAT_DF[RAT_DF['subject_id']==subject]  # restrict dataset to single subject\n",
        "    df = df[:first]  # restrict to \"first\" trials of data\n",
        "    # remove sessions with fewer than \"cutoff\" valid trials\n",
        "    df = df.groupby('session').filter(lambda x: len(x) >= cutoff)   \n",
        "\n",
        "    # Normalize the stimuli to standard normal\n",
        "    s_a = (df[\"s_a\"] - np.mean(df[\"s_a\"]))/np.std(df[\"s_a\"])\n",
        "    s_b = (df[\"s_b\"] - np.mean(df[\"s_b\"]))/np.std(df[\"s_b\"])\n",
        "    \n",
        "    # Determine which trials do not have a valid previous trial (mistrial or session boundary)\n",
        "    t = np.array(df[\"trial\"])\n",
        "    prior = ((t[1:] - t[:-1]) == 1).astype(int)\n",
        "    prior = np.hstack(([0], prior))\n",
        "\n",
        "    # Calculate previous average tone value\n",
        "    s_avg = (df[\"s_a\"][:-1] + df[\"s_b\"][:-1])/2\n",
        "    s_avg = (s_avg - np.mean(s_avg))/np.std(s_avg)\n",
        "    s_avg = np.hstack(([0], s_avg))\n",
        "    s_avg = s_avg * prior  # for trials without a valid previous trial, set to 0\n",
        "\n",
        "    # Calculate previous correct answer\n",
        "    h = (df[\"correct_side\"][:-1] * 2 - 1).astype(int)   # map from (0,1) to (-1,1)\n",
        "    h = np.hstack(([0], h))\n",
        "    h = h * prior  # for trials without a valid previous trial, set to 0\n",
        "    \n",
        "    # Calculate previous choice\n",
        "    c = (df[\"choice\"][:-1] * 2 - 1).astype(int)   # map from (0,1) to (-1,1)\n",
        "    c = np.hstack(([0], c))\n",
        "    c = c * prior  # for trials without a valid previous trial, set to 0\n",
        "    \n",
        "    inputs = dict(s_a = np.array(s_a)[:, None],\n",
        "                  s_b = np.array(s_b)[:, None],\n",
        "                  s_avg = np.array(s_avg)[:, None],\n",
        "                  h = np.array(h)[:, None],\n",
        "                  c = np.array(c)[:, None])\n",
        "\n",
        "    dat = dict(\n",
        "        subject = subject,\n",
        "        inputs = inputs,\n",
        "        s_a = np.array(df['s_a']),\n",
        "        s_b = np.array(df['s_b']),\n",
        "        correct = np.array(df['hit']),\n",
        "        answer = np.array(df['correct_side']),\n",
        "        y = np.array(df['choice']),\n",
        "        dayLength=np.array(df.groupby(['session']).size()),\n",
        "    )\n",
        "    return dat\n",
        "\n",
        "def getMouse(subject, p=5):\n",
        "    df = MOUSE_DF[MOUSE_DF['subject']==subject]   # Restrict data to the subject specified\n",
        "    \n",
        "    cL = np.tanh(p*df['contrastLeft'])/np.tanh(p)   # tanh transformation of left contrasts\n",
        "    cR = np.tanh(p*df['contrastRight'])/np.tanh(p)  # tanh transformation of right contrasts\n",
        "    inputs = dict(cL = np.array(cL)[:, None], cR = np.array(cR)[:, None])\n",
        "\n",
        "    dat = dict(\n",
        "        subject=subject,\n",
        "        lab=np.unique(df[\"lab\"])[0],\n",
        "        contrastLeft=np.array(df['contrastLeft']),\n",
        "        contrastRight=np.array(df['contrastRight']),\n",
        "        date=np.array(df['date']),\n",
        "        dayLength=np.array(df.groupby(['date','session']).size()),\n",
        "        correct=np.array(df['feedbackType']),\n",
        "        answer=np.array(df['answer']),\n",
        "        probL=np.array(df['probabilityLeft']),\n",
        "        inputs = inputs,\n",
        "        y = np.array(df['choice'])\n",
        "    )\n",
        "    \n",
        "    return dat\n",
        "\n",
        "def convertToDictRat(allDat, subject, first=20000, cutoff=50):\n",
        "\n",
        "    '''\n",
        "    equivalent to the function getRat from the paper see above and here https://tinyurl.com/PsyTrack-colab\n",
        "    '''\n",
        "\n",
        "    df = allDat[allDat['sID']==subject]  # restrict dataset to single subject\n",
        "    df = df[:first] # restrict to \"first\" trials of data\n",
        "    # # remove sessions with fewer than \"cutoff\" valid trials\n",
        "    # df = df.groupby('session').filter(lambda x: len(x) >= cutoff)   \n",
        "\n",
        "    # Determine which trials do not have a valid previous trial (mistrial or session boundary)\n",
        "    t = np.array(df[\"trial\"])\n",
        "    prior = ((t[1:] - t[:-1]) == 1).astype(int)\n",
        "    prior = np.hstack(([0], prior))\n",
        "\n",
        "    # Calculate previous average tone value\n",
        "    s_avg = df['Go/NoGo'][:-1]\n",
        "    s_avg = (s_avg - np.mean(s_avg))/np.std(s_avg)\n",
        "    s_avg = np.hstack(([0], s_avg))\n",
        "    s_avg = s_avg * prior  # for trials without a valid previous trial, set to 0\n",
        "\n",
        "    # Calculate previous correct answer\n",
        "    h = (df[\"Correct?\"][:-1] * 2 - 1).astype(int)   # map from (0,1) to (-1,1)\n",
        "    h = np.hstack(([0], h))\n",
        "    h = h * prior  # for trials without a valid previous trial, set to 0\n",
        "    \n",
        "    # Calculate previous choice\n",
        "    c = (df[\"choice\"][:-1] * 2 - 1).astype(int)   # map from (0,1) to (-1,1)\n",
        "    c = np.hstack(([0], c))\n",
        "    c = c * prior  # for trials without a valid previous trial, set to 0\n",
        "    \n",
        "    ## note here that it could be useful to have different stimulus values \n",
        "    ## important to respect the dictionary psy.COLORS hence the name of the specific names of the inputs\n",
        "    inputs = dict(s1 = np.array(df['Go/NoGo'])[:, None],\n",
        "                  s_avg = np.array(s_avg)[:, None], \n",
        "                  h = np.array(h)[:, None],\n",
        "                  c = np.array(c)[:, None])\n",
        "\n",
        "    dat = dict(\n",
        "        subject = subject,\n",
        "        inputs = inputs,\n",
        "        s1 = np.array(df['Go/NoGo']), # correspond to the go/noGo stim\n",
        "        correct = np.array(df['hit']), # hit correspond to hit and correct rejection\n",
        "        answer = np.array(df['Correct?']), #this is the answer \n",
        "        y = np.array(df['choice']), #this correspond to the Lick\n",
        "        dayLength=np.array(df.groupby(['session']).size()),\n",
        "    )\n",
        "\n",
        "    return dat\n",
        "\n",
        "def convertToDictMouse(allDat, subject, first=20000, cutoff=50):\n",
        "\n",
        "    '''\n",
        "    equivalent to the function getRat from the paper see above and here https://tinyurl.com/PsyTrack-colab\n",
        "    '''\n",
        "\n",
        "    df = allDat[allDat['sID']==subject]  # restrict dataset to single subject\n",
        "    df = df[:first] # restrict to \"first\" trials of data\n",
        "    # # remove sessions with fewer than \"cutoff\" valid trials\n",
        "    # df = df.groupby('session').filter(lambda x: len(x) >= cutoff)   \n",
        "\n",
        "    # Determine which trials do not have a valid previous trial (mistrial or session boundary)\n",
        "    t = np.array(df[\"trial\"])\n",
        "    prior = ((t[1:] - t[:-1]) == 1).astype(int)\n",
        "    prior = np.hstack(([0], prior))\n",
        "\n",
        "    # Calculate previous correct answer\n",
        "    h = (df[\"Correct?\"][:-1] * 2 - 1).astype(int)   # map from (0,1) to (-1,1)\n",
        "    h = np.hstack(([0], h))\n",
        "    h = h * prior  # for trials without a valid previous trial, set to 0\n",
        "    \n",
        "    # Calculate previous choice\n",
        "    c = (df[\"choice\"][:-1] * 2 - 1).astype(int)   # map from (0,1) to (-1,1)\n",
        "    c = np.hstack(([0], c))\n",
        "    c = c * prior  # for trials without a valid previous trial, set to 0\n",
        "    \n",
        "    # note here that it could be useful to have different stimulus values \n",
        "    inputs = dict(stim = np.array(df['Go/NoGo'])[:, None], \n",
        "                  h = np.array(h)[:, None],\n",
        "                  c = np.array(c)[:, None])\n",
        "\n",
        "    dat = dict(\n",
        "        subject = subject,\n",
        "        inputs = inputs,\n",
        "        stim = np.array(df['Go/NoGo']), # correspond to the go/noGo stim\n",
        "        correct = np.array(df['hit']), # hit correspond to hit and correct rejection\n",
        "        answer = np.array(df['Correct?']), #this is the answer \n",
        "        y = np.array(df['choice']), #this correspond to the Lick\n",
        "        dayLength=np.array(df.groupby(['session']).size()),\n",
        "    )\n",
        "\n",
        "    return dat\n",
        "\n",
        "def tpath(mypath, shareDrive = 'Y'):\n",
        "    '''\n",
        "    path conversion to switch form linux to windows platform with define drive\n",
        "    Args:\n",
        "    mypath (str): path of the file of interest\n",
        "    shareDrive (str): windows letter of the shared folder\n",
        "    '''\n",
        "    if ('google.colab' in str(get_ipython())) or sys.platform == 'win32':\n",
        "         myRoot = shareDrive+':'      \n",
        "    else:\n",
        "        myRoot = '/run/user/1000/gvfs/smb-share:server=ishtar,share=millerrumbaughlab'\n",
        "\n",
        "\n",
        "    newpath = myRoot+os.sep+mypath\n",
        "\n",
        "    return newpath\n",
        "\n",
        "def psyCompute(allDat, SPATH, sID, figure_off = False):\n",
        "    fname = SPATH+os.sep+str(sID)+'_fig5b_data.npz'\n",
        "\n",
        "    ## either load or generate the data\n",
        "    if glob.glob(fname) == [fname]:\n",
        "        dat = np.load(fname, allow_pickle=True)['dat'].item()\n",
        "    else:\n",
        "        ## convert the data\n",
        "        outData = convertToDictRat(allDat, sID)\n",
        "        new_dat = psy.trim(outData, START=0, END=12500)\n",
        "\n",
        "        # here weights could be adjusted \n",
        "        weights = {'bias': 1, 's1': 1, 'h': 1, 'c': 1, 's_avg':1}\n",
        "        K = np.sum([weights[i] for i in weights.keys()])\n",
        "        # hyper guess are kept with default value as in the paper\n",
        "        hyper_guess = {\n",
        "         'sigma'   : [2**-5]*K,\n",
        "         'sigInit' : 2**5,\n",
        "         'sigDay'  : [2**-4]*K,\n",
        "          }\n",
        "        optList = ['sigma', 'sigDay']\n",
        "\n",
        "        hyp, evd, wMode, hess_info = psy.hyperOpt(new_dat, hyper_guess, weights, optList)\n",
        "\n",
        "        dat = {'hyp' : hyp, 'evd' : evd, 'wMode' : wMode, 'W_std' : hess_info['W_std'],\n",
        "               'weights' : weights, 'new_dat' : new_dat}\n",
        "\n",
        "        # Save interim result\n",
        "        np.savez_compressed(SPATH+os.sep+str(sID)+'_fig5b_data.npz', dat=dat)\n",
        "\n",
        "    if figure_off == False:\n",
        "        print('graph')\n",
        "        # save the figure\n",
        "        fig = psy.plot_weights(dat['wMode'], dat['weights'], days=dat['new_dat'][\"dayLength\"], \n",
        "                               errorbar=dat['W_std'], figsize=(4.75,1.4))\n",
        "        # plt.xlabel(None); plt.ylabel(None)\n",
        "        # plt.subplots_adjust(0,0,1,1) \n",
        "        plt.savefig(SPATH +os.sep+str(sID)+ \"Fig5b.pdf\")\n",
        "        plt.close('all')\n",
        "\n",
        "def plot_all(all_labels, all_w, Weights, figsize):\n",
        "    fig = plt.figure(figsize=figsize)\n",
        "    Weights = [Weights] if type(Weights) is str else Weights\n",
        "    avg_len=6000 # this needs to be truncated for the average for the array to have same dimensions\n",
        "    for i, W in enumerate(Weights):\n",
        "        print(i,W)\n",
        "        avg = []\n",
        "        for i in np.arange(0,len(all_w),1):\n",
        "            print(i)\n",
        "            bias_ind = np.where(all_labels[i] == W)[0][-1]\n",
        "            bias_w = all_w[i][bias_ind]\n",
        "            avg += [list(bias_w[:avg_len]) + [np.nan]*(avg_len - len(bias_w[:avg_len]))]\n",
        "            colors = psy.COLORS\n",
        "            plt.plot(bias_w, color=colors[W], alpha=0.2, lw=1, zorder=2+i)\n",
        "        plt.plot(np.nanmean(avg, axis=0), color=colors[W], alpha=0.8, lw=2.5, zorder=5+i)\n",
        "\n",
        "    plt.axhline(0, color=\"black\", linestyle=\"--\", lw=1, alpha=0.5, zorder=1)\n",
        "    plt.tight_layout()\n",
        "    # plt.gca().spines['right'].set_visible(False)\n",
        "    # plt.gca().spines['top'].set_visible(False)\n",
        "    # plt.xlim(0, 6000)\n",
        "    # plt.ylim(-2.5, 2.5)\n",
        "    return fig\n",
        "\n",
        "def plotLabelsandW(genoVar, SPATH):\n",
        "    '''\n",
        "    this function will output all the labels and wheight for a givien genotype\n",
        "    based on the corresponding files and select the genotype of interest\n",
        "\n",
        "    args:\n",
        "    corresp(pd.DataFrame): data frame with file path sID and geno\n",
        "    geno(str): geno of intrest either 'wt' or 'het'\n",
        "    '''\n",
        "    \n",
        "    ## have the corresponding file generatede \n",
        "    npzFiles = glob.glob(SPATH+os.sep+'*.npz')\n",
        "    corresp = pd.DataFrame({'fname':npzFiles})\n",
        "    corresp['sID'] = corresp['fname'].str.split(os.sep).str[-1].str.split('_').str[0].astype(int)\n",
        "   \n",
        "    ## check and implement the genotypes\n",
        "    # try: geno\n",
        "    # except: geno = None\n",
        "    # if geno is None:\n",
        "    geno = pd.read_csv(os.sep.join(SPATH.split(os.sep)[:-1])+os.sep+'animals.csv')\n",
        "\n",
        "    corresp = pd.merge(corresp, geno, on='sID')\n",
        "\n",
        "    ## \n",
        "    cWTorHet = corresp[corresp['geno']==genoVar]\n",
        "    \n",
        "    all_labels = []\n",
        "    all_w = []\n",
        "\n",
        "    for i,j in cWTorHet.iterrows():\n",
        "        print(i,j)\n",
        "        rat = np.load(j['fname'], allow_pickle=True)['dat'].item()\n",
        "        \n",
        "        labels = []\n",
        "        for j in sorted(rat['weights'].keys()):\n",
        "            labels += [j]*rat['weights'][j]\n",
        "            \n",
        "        all_labels += [np.array(labels)]\n",
        "        all_w += [rat['wMode']] \n",
        "\n",
        "\n",
        "    myFigsize = (3.6,1.8)\n",
        "    plot_all(all_labels, all_w, [\"s1\"], myFigsize)\n",
        "    plt.ylim(-1, 15)\n",
        "    # plt.subplots_adjust(0,0,1,1) \n",
        "    # plt.gca().set_yticks([-2,0,2])\n",
        "    # plt.gca().set_xticklabels([])\n",
        "    plt.savefig(SPATH +os.sep+ genoVar+ \"Fig6a.pdf\")\n",
        "\n",
        "    plot_all(all_labels, all_w, [\"bias\"], myFigsize)\n",
        "    plt.ylim(-7, 2)\n",
        "    # plt.gca().set_yticks([-2,0,2])\n",
        "    # plt.gca().set_xticklabels([])\n",
        "    # plt.gca().set_yticklabels([])\n",
        "    # plt.subplots_adjust(0,0,1,1) \n",
        "    plt.savefig(SPATH +os.sep+ genoVar+ \"Fig6b.pdf\")\n",
        "\n",
        "    plot_all(all_labels, all_w, [\"h\"], myFigsize)\n",
        "    plt.ylim(-1, 1)\n",
        "    # # plt.gca().set_yticklabels([])\n",
        "    # plt.subplots_adjust(0,0,1,1) \n",
        "    plt.savefig(SPATH +os.sep+ genoVar+ \"Fig6d.pdf\")\n",
        "\n",
        "\n",
        "    plot_all(all_labels, all_w, [\"c\"], myFigsize)\n",
        "    plt.ylim(-1, 1)\n",
        "    # plt.gca().set_yticklabels([])\n",
        "    # plt.gca().set_xticklabels([])\n",
        "    # plt.subplots_adjust(0,0,1,1) \n",
        "    plt.savefig(SPATH +os.sep+ genoVar+ \"Fig6e.pdf\")\n"
      ],
      "metadata": {
        "id": "yzI1Ew69CwpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WDIL data retrieval"
      ],
      "metadata": {
        "id": "sA4RZwl9CzoF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) generate a list of files for the experiment\n",
        "\n",
        "This list can then be edited while keeping the original data and will:\n",
        "\n",
        "\n",
        "*   enable coding of missing data\n",
        "*   add detail on experimental parameters of stimulation\n",
        "  * phase (code for specific stimulus protocol to define)\n",
        "  * lick rate\n",
        "  * peak pupil\n",
        "\n",
        "Also need:\n",
        "\n",
        "\n",
        "*   list of genotypes named `animals.csv`\n",
        "*   list of what animals met criteria \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xdx092UjGet9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cohorts = [tpath(r'Sheldon\\All_WDIL\\WDIL009_EMXcreRUM2_7-20-21\\WDIL009_forpsytrack'),  tpath(r'Sheldon\\All_WDIL\\for psytrack\\WDIL007Box1+2'), tpath(r'Sheldon\\All_WDIL\\for psytrack\\WDIL010Box1+2')] # list of all the path and cohort of interest\n",
        "# mypath = cohorts[1]/\n",
        "\n",
        "### create and check for the full file list\n",
        "for mypath in cohorts:\n",
        "    print(mypath)\n",
        "    SPATH =  mypath+os.sep+'output'\n",
        "    os.makedirs(SPATH, exist_ok = True)\n",
        "\n",
        "    ## create a list of files \n",
        "    listFiles(mypath)\n",
        "\n",
        "    ## load or generate the data\n",
        "    # if glob.glob(mypath+os.sep+'allDat.csv') == [mypath+os.sep+'allDat.csv']:\n",
        "    #     allDat = pd.read_csv(mypath+os.sep+'allDat.csv')\n",
        "    # else:\n",
        "    #     allDat = formatWDILfile(mypath)\n",
        "    #     allDat.to_csv(mypath+os.sep+'allDat.csv')\n",
        "\n"
      ],
      "metadata": {
        "id": "zfkSC77vhfxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2) get data from csv file list"
      ],
      "metadata": {
        "id": "m-El4uzoKr_4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part is important and deal with missing data. In practice it is better to drop the data than to replace them in this specific context.\n"
      ],
      "metadata": {
        "id": "tn4E4rAmi53C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## coding dat file\n",
        "mypath = cohorts[1]\n",
        "SPATH =  mypath+os.sep+'output'\n",
        "if glob.glob(mypath+os.sep+'allDat.csv') == [mypath+os.sep+'allDat.csv']:\n",
        "    allDat = pd.read_csv(mypath+os.sep+'allDat.csv')\n",
        "else:\n",
        "    allDat = formatWDILfile(mypath)\n",
        "    allDat.to_csv(mypath+os.sep+'allDat.csv')"
      ],
      "metadata": {
        "id": "Yi1wjD40JDBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Dealing with NaN\n",
        "## important dispaly item for double check\n",
        "## dealing with potential missing data in the folder\n",
        "print(allDat[allDat.isnull().any(axis=1)])\n",
        "allDat = allDat.dropna()"
      ],
      "metadata": {
        "id": "nfY4G7lljB_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Coding of the file in a proper format for further processing"
      ],
      "metadata": {
        "id": "HkdPWXW3jQAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "allDat = codingDatFile(allDat)\n",
        "## add genotype info to the file\n",
        "## adding the genotype to the dat file\n",
        "## then as a first pass can split the file and process wt or het\n",
        "geno = pd.read_csv(mypath+os.sep+\"animals.csv\")\n",
        "allDat = pd.merge(allDat, geno, on ='sID')"
      ],
      "metadata": {
        "id": "SN7xdIFQjQkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test data WDIL007 sID 1753"
      ],
      "metadata": {
        "id": "3z_FGuY2pcfR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sID = 1753 #input the idname of the subeject\n",
        "fname = SPATH+os.sep+str(sID)+'_fig5b_data.npz'\n",
        "psyCompute(allDat, sID)"
      ],
      "metadata": {
        "id": "G847i0J2yqqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test data all WDIL007 - corresponding to Fig6"
      ],
      "metadata": {
        "id": "Zb-5bPz3yg-s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) generate all the model by individual\n",
        "*takes roughly 40 min for 12 animals in our data sets on 6 core CPU. It depends a lot of th enumber of trials and parameters to model*"
      ],
      "metadata": {
        "id": "DV3FEGnKJNsg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## takes roughly 30 min with 6 core CPU\n",
        "## length highly dependent on number of trials etc.\n",
        "all_id = allDat['sID'].unique()\n",
        "a = time.time()\n",
        "for i, sID in enumerate(all_id):\n",
        "    print(i, sID)\n",
        "    try:\n",
        "        psyCompute(allDat, SPATH, sID) ## psyCompute is already parallelized on CPU thus the more cpu the better\n",
        "    except:\n",
        "        print('error with: ', sID)\n",
        "b = time.time()\n",
        "print(b-a)"
      ],
      "metadata": {
        "id": "xUyNVM68ynUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) generate graphs base on specific criteria"
      ],
      "metadata": {
        "id": "xJT4k-4tJUwN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in ['wt', 'het']:\n",
        "    plotLabelsandW(geno=i, SPATH)\n"
      ],
      "metadata": {
        "id": "npvAbn0aj45e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}